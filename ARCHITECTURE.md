# TalentAI - BERT-T5 Hybrid Framework Architecture

## ğŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [Component Details](#component-details)
4. [Data Flow](#data-flow)
5. [Technical Implementation](#technical-implementation)
6. [Configuration & Scoring Physics](#configuration--scoring-physics)
7. [User Interfaces](#user-interfaces)
8. [File Structure](#file-structure)

---

## ğŸ¯ Project Overview

**TalentAI** is an AI-powered recruitment framework that combines two state-of-the-art transformer models to solve two critical problems in talent acquisition:

1. **Job Description Generation** - Automatically create compelling, constraint-aware job descriptions using T5
2. **Candidate Ranking** - Intelligently match and rank candidates against job descriptions using BERT

### Why This Architecture?

#### The Dual-Model Approach

**T5 (Text-to-Text Transfer Transformer) - The Generator**
- **What it does**: Generates human-like job descriptions from structured inputs
- **Why T5**: Excels at text generation tasks due to its encoder-decoder architecture
- **Key advantage**: Can be fine-tuned on job description datasets to learn industry-specific language patterns

**BERT (Bidirectional Encoder Representations from Transformers) - The Ranker**
- **What it does**: Scores resume-JD pairs for semantic similarity
- **Why BERT**: Superior at understanding contextual relationships between texts
- **Key advantage**: Cross-encoder architecture allows deep interaction between resume and JD tokens

### Core Innovation: Hybrid Scoring

Unlike traditional keyword-matching systems, this framework uses a **multi-dimensional scoring equation**:

```
Score = Î± Ã— Semantic_Match - Î² Ã— Skill_Penalty - Î³ Ã— Experience_Gap
```

This allows recruiters to tune the balance between:
- **Potential vs. Requirements** (adjust Î±)
- **Strict skill matching vs. Flexibility** (adjust Î²)
- **Experience level importance** (adjust Î³)

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "Input Layer"
        A[User Input: Role + Skills + Experience]
        B[Resume Database CSV]
    end
    
    subgraph "Generation Pipeline"
        C[T5 Generator]
        D[Prompt Constructor]
        E[Generated JD]
    end
    
    subgraph "Ranking Pipeline"
        F[BERT Ranker]
        G[Tokenization]
        H[Cross-Encoder Scoring]
        I[Hybrid Score Calculator]
    end
    
    subgraph "Explainability Layer"
        J[Attention Mechanism]
        K[Token-Level Analysis]
        L[Contrastive Comparison]
    end
    
    subgraph "Output Layer"
        M[Ranked Candidates]
        N[Attention Explanations]
        O[Metrics Dashboard]
    end
    
    A --> D
    D --> C
    C --> E
    E --> G
    B --> G
    G --> H
    H --> I
    I --> M
    F --> J
    J --> K
    K --> N
    J --> L
    L --> N
    M --> O
    N --> O
    
    style A fill:#e1f5ff
    style E fill:#d4edda
    style M fill:#fff3cd
    style N fill:#f8d7da
```

---

## ğŸ”§ Component Details

### 1. T5 Job Description Generator (`models/t5_generator.py`)

#### Purpose
Converts structured job requirements into natural, human-readable job descriptions.

#### How It Works

**Step 1: Prompt Construction**
```
Input: role = "Data Scientist", skills = "Python, SQL", experience = 5
Prompt: "generate job description: role: Data Scientist mandatory: Python, SQL experience: 5 years"
```

**Why structured prompts?**
- Ensures consistent generation patterns
- Allows the model to learn specific mappings
- Enables constraint adherence tracking

**Step 2: Text Generation**
- Uses beam search (num_beams=4) to explore multiple generation paths
- Applies no_repeat_ngram_size=2 to prevent repetitive phrases
- Max length of 300 tokens balances detail vs. readability

**Step 3: Fine-tuning (Optional)**
```python
def train(self, dataset, epochs=1, batch_size=4, lr=1e-4)
```
- Trains on job posting datasets to learn industry-specific language
- Uses AdamW optimizer for stable convergence
- Batch processing for memory efficiency

#### Why This Matters
- **Time savings**: Generate JDs in seconds vs. hours of manual writing
- **Consistency**: Maintains company tone and structure
- **Constraint adherence**: Ensures mandatory skills are mentioned

---

### 2. BERT Candidate Ranker (`models/bert_ranker.py`)

#### Purpose
Scores how well a resume matches a job description using deep semantic understanding.

#### Architecture Details

**Cross-Encoder Design**
```
Input: "[CLS] Resume Text [SEP] JD Text [SEP]"
       â†“
    BERT Encoder (12 layers, 768 hidden units)
       â†“
    [CLS] Token Embedding (768-dimensional vector)
       â†“
    Linear Scorer Head (768 â†’ 1)
       â†“
    Sigmoid Activation (0-1 score)
```

**Why Cross-Encoder vs. Siamese?**
- **Cross-Encoder**: Resume and JD interact at every layer â†’ better accuracy
- **Siamese**: Separate encoding â†’ faster but less accurate
- **Trade-off**: We chose accuracy over speed for quality hiring decisions

#### Hybrid Scoring Formula

```python
def get_research_score(self, resume_text, jd_text, mandatory_skills, experience_gap, Î±, Î², Î³):
    # 1. Semantic Similarity (BERT-based)
    sim_score = self.predict(resume_text, jd_text)  # 0-1 range
    
    # 2. Skill Coverage Penalty
    missing_skills = count_missing_skills(resume_text, mandatory_skills)
    penalty_missing = missing_skills / total_mandatory_skills
    
    # 3. Experience Gap Penalty
    penalty_exp = min(1.0, experience_gap / 10.0)  # Normalized to 0-1
    
    # Final Score
    return Î± Ã— sim_score - Î² Ã— penalty_missing - Î³ Ã— penalty_exp
```

**Component Breakdown:**

| Component | Range | Meaning | Why It Exists |
|-----------|-------|---------|---------------|
| **Î± Ã— Semantic Match** | 0-1 | How well the resume meaning aligns with JD requirements | Captures soft skills, cultural fit, transferable experience |
| **Î² Ã— Skill Penalty** | 0-1 | Fraction of mandatory skills missing | Ensures hard requirements are met (e.g., "Must know Java") |
| **Î³ Ã— Experience Gap** | 0-1 | Years of experience shortfall (capped at 10) | Balances junior vs. senior role requirements |

**Example Calculation:**
```
Candidate A:
- Semantic Match: 0.85
- Missing Skills: 1/5 = 0.20
- Experience Gap: 2/10 = 0.20

With Î±=0.7, Î²=0.2, Î³=0.1:
Score = 0.7Ã—0.85 - 0.2Ã—0.20 - 0.1Ã—0.20
      = 0.595 - 0.04 - 0.02
      = 0.535 (53.5% match)
```

---

### 3. Attention-Based Explainability (`models/attention_utils.py`)

#### Purpose
Makes AI decisions transparent by showing which resume words influenced the ranking.

#### How Attention Works

**Conceptual Explanation:**
Think of BERT as a spotlight that can focus on different parts of the text. Attention weights tell us where the spotlight is shining brightest.

```
Resume: "5 years Python experience with Django and AWS deployments"
JD: "Senior Python Developer needed with cloud experience"

Attention Weights:
"Python" â†’ 0.234 (high - direct match)
"AWS" â†’ 0.198 (high - cloud experience)
"Django" â†’ 0.156 (medium - relevant framework)
"5 years" â†’ 0.112 (medium - seniority indicator)
"experience" â†’ 0.089 (low - common word)
```

#### Implementation Details

**1. Single Candidate Explanation** (`explain_prediction`)
```python
def explain_prediction(ranker, tokenizer, resume_text, jd_text):
    # Get attention matrices from all 12 BERT layers
    score, attentions = ranker.forward(..., output_attentions=True)
    
    # Focus on last layer (most refined understanding)
    last_layer = attentions[-1]  # Shape: [batch, heads, seq_len, seq_len]
    
    # Average over 12 attention heads
    avg_heads = last_layer.mean(dim=1)  # Shape: [batch, seq_len, seq_len]
    
    # Extract [CLS] token's attention to all other tokens
    cls_attention = avg_heads[0, 0, :]  # Shape: [seq_len]
    
    # Pair tokens with their weights
    return [(token, weight) for token, weight in zip(tokens, cls_attention)]
```

**Why the [CLS] token?**
- BERT's [CLS] token aggregates information from the entire sequence
- Its attention pattern shows what the model considers important for classification

**2. Contrastive Comparison** (`compare_candidates`)
```python
def compare_candidates(ranker, tokenizer, jd_text, resume_A, resume_B):
    # Get attention maps for both candidates
    map_A = get_attention_map(resume_A)
    map_B = get_attention_map(resume_B)
    
    # Calculate contrast scores
    for token in map_A:
        contrast = map_A[token] - map_B[token]  # Positive = A's advantage
    
    return top_contrasting_tokens
```

**Use Case:**
"Why did Sarah score higher than John?"
```
Advantages for Sarah:
- "kubernetes" â†’ +0.145 (Sarah has it, John doesn't)
- "terraform" â†’ +0.098 (stronger experience)
- "5 years" â†’ +0.067 (more experience)
```

---

### 4. Metrics & Validation (`utils/metrics.py`)

#### Skill Adherence Metric
```python
def calculate_skill_adherence(generated_jd, required_skills):
    """
    Measures: "Did the T5 model include all required skills in the generated JD?"
    """
    skills_found = count_mentioned_skills(generated_jd, required_skills)
    return skills_found / total_required_skills
```

**Why this matters:**
- Detects model hallucination (making up skills)
- Ensures compliance with job requirements
- Validates generation quality

#### Constraint Violation Metric (CVM)
```python
def calculate_cvm(generated_jd, mandatory_skills):
    """
    Inverse of adherence - higher CVM = worse generation
    CVM = 0.0 means perfect adherence
    CVM = 1.0 means no skills mentioned
    """
    missing_skills = count_missing_skills(generated_jd, mandatory_skills)
    return missing_skills / total_mandatory_skills
```

**Practical Example:**
```
Generated JD for "Python, Django, AWS" role
Contains: "Python, Django, Docker"

Adherence = 2/3 = 66.7%
CVM = 1/3 = 33.3% (missing AWS)
```

---

### 5. Data Pipeline (`data/dataset_loader.py`)

#### Purpose
Handles messy real-world job posting data and prepares it for model training.

#### Dataset Schema Normalization

**Problem:** Different data sources use different column names
```
Source 1: "Job Title", "Description", "Skills Required"
Source 2: "title", "job_description", "job_skill_set"
Source 3: "Role", "JD", "Required Skills"
```

**Solution:** Automatic column mapping
```python
# Normalize column names
data.columns = [c.lower().replace(' ', '_') for c in data.columns]

# Map to standard names
if 'title' in columns: rename to 'job_title'
if 'description' in columns: rename to 'job_description'
if 'skills' in columns: rename to 'job_skill_set'
```

#### Skill Extraction Fallback

**Problem:** Some job postings don't have explicit skills listed

**Solution:** Heuristic extraction
```python
def extract_skills(description):
    # 1. Try to find skills section
    match = re.search(r'(skills|requirements)[:\n]', description)
    if match:
        return description[match.end():match.end()+300]
    
    # 2. Fallback: Keyword matching
    keywords = ["python", "java", "sql", "aws", ...]
    found = [k for k in keywords if k in description.lower()]
    return ", ".join(found)
```

#### T5 Training Data Format
```python
def __getitem__(self, idx):
    # Input: Structured prompt
    input_text = f"generate job description: role: {title} skills: {skills}"
    
    # Target: Actual job description from dataset
    target_text = description
    
    # Tokenize for T5
    return {
        "input_ids": tokenized_input,
        "attention_mask": attention_mask,
        "labels": tokenized_target  # What we want T5 to generate
    }
```

---

## ğŸ”„ Data Flow

### Complete Pipeline Walkthrough

#### **Phase 1: Job Description Generation**

```
User Input:
â”œâ”€â”€ Role: "Senior Backend Engineer"
â”œâ”€â”€ Mandatory Skills: "Python, Django, AWS"
â”œâ”€â”€ Optional Skills: "Kubernetes, PostgreSQL"
â””â”€â”€ Experience: 8 years

     â†“

Prompt Construction:
"generate job description: role: Senior Backend Engineer 
 mandatory: Python, Django, AWS 
 optional: Kubernetes, PostgreSQL 
 experience: 8 years"

     â†“

T5 Model Processing:
1. Tokenize prompt â†’ Input IDs
2. Encode with T5 encoder (6 layers)
3. Generate with T5 decoder (beam search)
4. Decode tokens â†’ Text

     â†“

Generated Output:
"We are seeking a Senior Backend Engineer with 8+ years of experience.
 Strong proficiency in Python and Django required for building scalable APIs.
 AWS deployment experience essential. Knowledge of Kubernetes and PostgreSQL
 is a plus. You will work on high-traffic systems..."

     â†“

Validation:
â”œâ”€â”€ Skill Adherence: 100% (all mandatory skills present)
â”œâ”€â”€ CVM: 0.0 (no violations)
â””â”€â”€ Length: 287 tokens
```

#### **Phase 2: Candidate Ranking**

```
Generated JD + Resume Database (CSV)
     â†“
For each resume:
     â”œâ”€â”€ Extract: Skills, Experience, Education
     â”œâ”€â”€ Construct Resume Text
     â””â”€â”€ Pass to BERT Ranker
          â†“
     Hybrid Scoring:
          â”œâ”€â”€ Î± Ã— BERT Semantic Score
          â”œâ”€â”€ - Î² Ã— Skill Penalty
          â””â”€â”€ - Î³ Ã— Experience Gap
               â†“
          Final Score: 0.623
     â†“
Sort by Score (descending)
     â†“
Top 5 Candidates:
1. Alice Chen - 0.728
2. Bob Kumar - 0.623
3. Carol Lee - 0.591
4. David Smith - 0.487
5. Eve Johnson - 0.412
```

#### **Phase 3: Explainability**

```
Top Candidate (Alice Chen)
     â†“
Get BERT Attention Weights
     â†“
Extract [CLS] Token Attention
     â†“
Top Influential Terms:
â”œâ”€â”€ "django" â†’ 0.234
â”œâ”€â”€ "aws" â†’ 0.198
â”œâ”€â”€ "python" â†’ 0.187
â”œâ”€â”€ "kubernetes" â†’ 0.156
â””â”€â”€ "8 years" â†’ 0.142
     â†“
Display to User (Transparent Decision)
```

---

## âš™ï¸ Configuration & Scoring Physics

### Configuration File (`config.yaml`)

```yaml
weights:
  alpha: 0.7   # Semantic Match Weight
  beta: 0.2    # Skill Penalty Weight
  gamma: 0.1   # Experience Gap Weight

models:
  t5_name: "t5-small"
  bert_name: "bert-base-uncased"

generation:
  max_length: 300
  num_beams: 4
```

### Weight Tuning Guide

#### **Scenario 1: Startup Hiring (Flexible)**
```yaml
alpha: 0.9   # High - value potential over perfection
beta: 0.05   # Low - open to learning on the job
gamma: 0.05  # Low - okay with less experience
```
**Result:** Prioritizes candidates with good fit and learning potential

#### **Scenario 2: Enterprise Critical Role**
```yaml
alpha: 0.5   # Medium - semantic match still matters
beta: 0.4    # High - must have all required skills
gamma: 0.1   # Low-Medium - experience matters less than skills
```
**Result:** Strict skill matching, less forgiving

#### **Scenario 3: Senior Leadership Position**
```yaml
alpha: 0.6   # Medium-High - cultural fit important
beta: 0.2    # Medium - some flexibility on exact skills
gamma: 0.2   # High - experience is crucial
```
**Result:** Balances experience with soft skills

### Weight Normalization

The system displays a "Total Weight" indicator:
```python
total = alpha + beta + gamma
status = "Normalized" if abs(total - 1.0) < 0.1 else "Unbalanced"
```

**Why normalize to 1.0?**
- Keeps scores in consistent 0-1 range
- Makes score comparison meaningful across configurations
- Prevents one factor from dominating

---

## ğŸ’» User Interfaces

### 1. Command-Line Interface (`main.py`)

**Use Case:** Quick testing, automation, batch processing

**Flow:**
```bash
$ python main.py "Data Scientist" "Python, SQL"

=== BERT-T5 Hybrid Framework ===
Target: Data Scientist | Skills: Python, SQL

[1/4] Generating Job Description...
Generated JD Preview:
We are looking for a Data Scientist with expertise in Python and SQL...
Skill Adherence: 100%

[2/4] Ranking Candidates from 'data/resumes.csv'...
Loaded 100 resumes.

[3/4] Top 5 Candidates:
1. Sarah Johnson (Data Analyst) - Match Score: 0.7234
2. Mike Chen (ML Engineer) - Match Score: 0.6891
3. Emily Davis (Data Scientist) - Match Score: 0.6543
4. Alex Kumar (Business Analyst) - Match Score: 0.5987
5. Lisa Wang (Statistician) - Match Score: 0.5432

[4/4] Explaining Top Match...
Candidate: Sarah Johnson
Top Influential Terms (Attention):
- python: 0.2341
- sql: 0.1987
- statistics: 0.1654
- pandas: 0.1432
- 5 years: 0.1123

=== Done ===
```

**Key Features:**
- Accepts CLI arguments: `python main.py <role> <skills>`
- Interactive prompts if no arguments
- Shows adherence metrics
- Provides explainability for top match

### 2. Streamlit Web Application (`app.py`)

**Use Case:** Interactive exploration, recruiter tool, demos

#### Tab 1: Generator
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job Description Generator  â”‚  Generated Preview â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Target Role:                â”‚                    â”‚
â”‚ [Senior Backend Engineer  ] â”‚  (Generated JD     â”‚
â”‚                             â”‚   appears here     â”‚
â”‚ Experience Required:        â”‚   after clicking   â”‚
â”‚ â”â”â”â”â”â”â”â—â”â”â”â”â”â”â” 8 years    â”‚   Generate)        â”‚
â”‚                             â”‚                    â”‚
â”‚ Mandatory Skills:           â”‚  Metrics:          â”‚
â”‚ [Python, Django, AWS      ] â”‚  - Adherence: 100% â”‚
â”‚                             â”‚  - Hallucination:  â”‚
â”‚ Optional Skills:            â”‚    0%              â”‚
â”‚ [Kubernetes, PostgreSQL]    â”‚                    â”‚
â”‚                             â”‚                    â”‚
â”‚ [ğŸš€ Generate Job Description]â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Tab 2: Ranker
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Candidate Rankings                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Candidate    â”‚ Global Scoreâ”‚ Semantic â”‚ Risk   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Alice Chen   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 73%â”‚ 85%      â”‚ 12%    â”‚
â”‚ Bob Kumar    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 62%  â”‚ 78%      â”‚ 16%    â”‚
â”‚ Carol Lee    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 59%   â”‚ 72%      â”‚ 13%    â”‚
â”‚ David Smith  â”‚ â–ˆâ–ˆâ–ˆâ–ˆ 49%    â”‚ 65%      â”‚ 16%    â”‚
â”‚ Eve Johnson  â”‚ â–ˆâ–ˆâ–ˆ 41%     â”‚ 58%      â”‚ 17%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Tab 3: X-Ray Analysis
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ X-Ray Analysis                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Comparison    â”‚ Token Advantages                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                 â”‚
â”‚ Candidate A:  â”‚    kubernetes â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â”‚
â”‚ [Alice Chenâ–¼] â”‚    terraform  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            â”‚
â”‚               â”‚    docker     â–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚ Candidate B:  â”‚    5 years    â–ˆâ–ˆâ–ˆ               â”‚
â”‚ [Bob Kumar  â–¼]â”‚    aws        â–ˆâ–ˆ                â”‚
â”‚               â”‚                                 â”‚
â”‚ [Run Analysis]â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Sidebar: Ranking Physics
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ§  TalentAI             â”‚
â”‚ v2.0 Enterprise Edition â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âš™ï¸ Ranking Physics      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Alpha (Semantic Match)  â”‚
â”‚ â”â”â”â”â”â”â”â—â”â”â” 0.7        â”‚
â”‚                         â”‚
â”‚ Beta (Skill Penalty)    â”‚
â”‚ â”â”â—â”â”â”â”â”â”â”â” 0.2        â”‚
â”‚                         â”‚
â”‚ Gamma (Exp. Gap)        â”‚
â”‚ â”â—â”â”â”â”â”â”â”â”â” 0.1        â”‚
â”‚                         â”‚
â”‚ Total Weight: 1.0       â”‚
â”‚ Status: âœ“ Normalized    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Backend: T5-Small +     â”‚
â”‚          BERT-Uncased   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Streamlit?**
- Rapid prototyping (built in ~200 lines)
- Built-in caching (`@st.cache_resource`)
- Interactive widgets with no JS required
- Easy deployment (Streamlit Cloud, Docker)

---

## ğŸ“ File Structure

```
bert_t5_jd_framework/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                      # CLI entry point
â”œâ”€â”€ ğŸ“„ app.py                       # Streamlit web interface
â”œâ”€â”€ ğŸ“„ config.yaml                  # Configurable weights/models
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md              # This file
â”‚
â”œâ”€â”€ ğŸ“ models/                      # Core AI components
â”‚   â”œâ”€â”€ ğŸ“„ t5_generator.py         # T5 job description generator
â”‚   â”œâ”€â”€ ğŸ“„ bert_ranker.py          # BERT resume ranker
â”‚   â””â”€â”€ ğŸ“„ attention_utils.py      # Explainability utilities
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Dataset management
â”‚   â”œâ”€â”€ ğŸ“„ dataset_loader.py       # PyTorch Dataset classes
â”‚   â”œâ”€â”€ ğŸ“„ resumes.csv             # Sample resume database
â”‚   â”œâ”€â”€ ğŸ“„ jobs_dataset.csv        # Job postings for training
â”‚   â””â”€â”€ ğŸ“„ job_skills.csv          # Skills taxonomy
â”‚
â”œâ”€â”€ ğŸ“ utils/                       # Helper functions
â”‚   â””â”€â”€ ğŸ“„ metrics.py              # Adherence & CVM calculations
â”‚
â”œâ”€â”€ ğŸ“ .streamlit/                  # Streamlit configuration
â”‚   â””â”€â”€ ğŸ“„ config.toml             # Theme & server settings
â”‚
â””â”€â”€ ğŸ“ scripts/                     # Training & testing
    â”œâ”€â”€ ğŸ“„ train_t5_debug.py       # T5 model training
    â”œâ”€â”€ ğŸ“„ train_bert_debug.py     # BERT model fine-tuning
    â”œâ”€â”€ ğŸ“„ run_batch_tests.py      # Batch evaluation
    â”œâ”€â”€ ğŸ“„ run_adversarial_tests.py # Robustness testing
    â””â”€â”€ ğŸ“„ run_correlation_study.py # Score analysis
```

---

## ğŸ”¬ Advanced Topics

### Model Training Workflow

#### T5 Fine-Tuning (`train_t5_debug.py`)
```python
# 1. Load dataset
dataset = JDDataset("data/jobs_dataset.csv", tokenizer, mode="generation")

# 2. Initialize T5
t5 = T5JDGenerator(model_name="t5-small")

# 3. Train
t5.train(dataset, epochs=3, batch_size=8, lr=1e-4)

# 4. Save
# Automatically saved to models/saved_t5/
```

**Training Data Format:**
```
Input: "generate job description: role: Data Engineer skills: Python, Spark, SQL"
Target: "We are looking for a Data Engineer to build ETL pipelines..."
```

#### BERT Fine-Tuning (`train_bert_debug.py`)
```python
# 1. Create training pairs
pairs = [
    ("Resume: Python, SQL, 3 years", "JD: Looking for Python developer", 0.9),  # Good match
    ("Resume: Java, C++, 5 years", "JD: Python developer needed", 0.3),         # Poor match
]

# 2. Train with cross-entropy loss
criterion = nn.BCELoss()
for epoch in range(epochs):
    for resumes, jds, labels in dataloader:
        loss = ranker.train_step(resumes, jds, labels, optimizer, criterion)
```

### Performance Optimization

**Model Caching (Streamlit)**
```python
@st.cache_resource
def load_models():
    # Loaded once, reused across sessions
    return T5JDGenerator(), BertRanker()
```

**GPU Acceleration**
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
```

**Batch Inference**
```python
# Instead of loop:
for resume in resumes:
    score = ranker.predict(resume, jd)

# Use batch processing:
scores = ranker.batch_predict(resumes, jd)
```

---

## ğŸ“ Key Takeaways

### Architectural Principles

1. **Separation of Concerns**
   - Generation (T5) and Ranking (BERT) are independent modules
   - Can swap models without changing pipeline logic

2. **Configurability**
   - Scoring weights in YAML for non-technical users
   - No code changes needed to tune behavior

3. **Explainability First**
   - Every prediction comes with attention-based reasoning
   - Builds trust in AI decisions

4. **Production-Ready Design**
   - CLI for automation
   - Web UI for human interaction
   - Metrics for quality monitoring

### When to Use This Architecture

âœ… **Good Fit:**
- Recruiting workflows with consistent job types
- Need for transparent AI decisions
- Balancing hard requirements vs. soft skills

âŒ **Not Ideal For:**
- One-off hiring (overhead not worth it)
- Purely keyword-based matching (simpler solutions exist)
- Real-time applications (inference is ~1-2s per candidate)

---

## ğŸ“š References & Resources

### Model Documentation
- [T5 Paper: "Exploring the Limits of Transfer Learning"](https://arxiv.org/abs/1910.10683)
- [BERT Paper: "Pre-training of Deep Bidirectional Transformers"](https://arxiv.org/abs/1810.04805)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)

### Related Concepts
- Cross-Encoder vs. Bi-Encoder architectures
- Beam search decoding
- Attention mechanism visualization
- Transfer learning in NLP

---

**Last Updated:** January 2026  
**Version:** 2.0  
**Maintainer:** TalentAI Research Team
